/**
 * LLM å®¢æˆ·ç«¯æŠ½è±¡
 * å…¼å®¹ OpenAI é£æ ¼çš„ Chat Completions API
 * æ”¯æŒé€šä¹‰åƒé—®ç­‰å…¼å®¹æ¥å£
 */

import { ChatMessage, LLMResponse } from "@/lib/types";
import { getConfig } from "@/lib/config";

export interface LLMClientOptions {
  baseUrl?: string;
  apiKey?: string;
  model?: string;
  temperature?: number;
  maxTokens?: number;
  timeout?: number;
}

/**
 * LLM å®¢æˆ·ç«¯ç±»
 */
export class LLMClient {
  private baseUrl: string;
  private apiKey: string;
  private model: string;
  private temperature: number;
  private maxTokens: number;
  private timeout: number;

  constructor(options?: LLMClientOptions) {
    const config = getConfig();
    
    this.baseUrl = options?.baseUrl || config.llm.baseUrl;
    this.apiKey = options?.apiKey || config.llm.apiKey;
    this.model = options?.model || config.llm.model;
    this.temperature = options?.temperature ?? 0.7;
    this.maxTokens = options?.maxTokens ?? 4000;
    this.timeout = options?.timeout ?? 60000;
  }

  /**
   * è°ƒç”¨ Chat Completions API
   */
  async chat(messages: ChatMessage[]): Promise<LLMResponse> {
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), this.timeout);

    try {
      const response = await fetch(`${this.baseUrl}/chat/completions`, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          Authorization: `Bearer ${this.apiKey}`,
        },
        body: JSON.stringify({
          model: this.model,
          messages,
          temperature: this.temperature,
          max_tokens: this.maxTokens,
        }),
        signal: controller.signal,
      });

      clearTimeout(timeoutId);

      if (!response.ok) {
        const errorText = await response.text();
        throw new Error(
          `LLM API è°ƒç”¨å¤±è´¥ (${response.status}): ${errorText}`
        );
      }

      const data = await response.json();

      if (!data.choices || data.choices.length === 0) {
        throw new Error("LLM API è¿”å›çš„å“åº”ä¸­æ²¡æœ‰ choices");
      }

      const content = data.choices[0].message?.content || "";
      const usage = data.usage || undefined;

      return {
        content,
        usage: usage
          ? {
              prompt_tokens: usage.prompt_tokens,
              completion_tokens: usage.completion_tokens,
              total_tokens: usage.total_tokens,
            }
          : undefined,
      };
    } catch (error: any) {
      clearTimeout(timeoutId);
      
      if (error.name === "AbortError") {
        throw new Error(`LLM API è°ƒç”¨è¶…æ—¶ (${this.timeout}ms)`);
      }
      
      throw error;
    }
  }

  /**
   * ç®€åŒ–çš„å•æ¬¡å¯¹è¯æ¥å£
   */
  async generate(prompt: string, systemPrompt?: string): Promise<LLMResponse> {
    const messages: ChatMessage[] = [];
    
    if (systemPrompt) {
      messages.push({ role: "system", content: systemPrompt });
    }
    
    messages.push({ role: "user", content: prompt });
    
    return this.chat(messages);
  }

  /**
   * å¸¦é‡è¯•çš„ç”Ÿæˆ
   */
  async generateWithRetry(
    prompt: string,
    systemPrompt?: string,
    maxRetries: number = 2
  ): Promise<LLMResponse> {
    let lastError: Error | null = null;
    
    console.log('\n' + '='.repeat(80));
    console.log('ğŸ“¤ LLM è¯·æ±‚è¯¦æƒ…');
    console.log('='.repeat(80));
    console.log('[LLM] æ¨¡å‹:', this.model);
    console.log('[LLM] Temperature:', this.temperature);
    console.log('[LLM] Max Tokens:', this.maxTokens);
    console.log('[LLM] System Prompt é•¿åº¦:', systemPrompt?.length || 0, 'å­—ç¬¦');
    console.log('[LLM] User Prompt é•¿åº¦:', prompt.length, 'å­—ç¬¦');
    console.log('-'.repeat(80));
    
    if (systemPrompt) {
      console.log('ğŸ“‹ System Prompt (å‰500å­—ç¬¦):');
      console.log(systemPrompt.substring(0, 500));
      if (systemPrompt.length > 500) {
        console.log('... (è¿˜æœ‰', systemPrompt.length - 500, 'å­—ç¬¦)');
      }
      console.log('-'.repeat(80));
    }
    
    console.log('ğŸ“ User Prompt (å‰1000å­—ç¬¦):');
    console.log(prompt.substring(0, 1000));
    if (prompt.length > 1000) {
      console.log('... (è¿˜æœ‰', prompt.length - 1000, 'å­—ç¬¦)');
    }
    console.log('='.repeat(80) + '\n');
    
    for (let i = 0; i <= maxRetries; i++) {
      try {
        const response = await this.generate(prompt, systemPrompt);
        
        console.log('\n' + '='.repeat(80));
        console.log('ğŸ“¥ LLM å“åº”è¯¦æƒ…');
        console.log('='.repeat(80));
        console.log('[LLM] å“åº”é•¿åº¦:', response.content.length, 'å­—ç¬¦');
        if (response.usage) {
          console.log('[LLM] Token ä½¿ç”¨:');
          console.log('  - Prompt Tokens:', response.usage.prompt_tokens);
          console.log('  - Completion Tokens:', response.usage.completion_tokens);
          console.log('  - Total Tokens:', response.usage.total_tokens);
        }
        console.log('-'.repeat(80));
        console.log('ğŸ’¬ å“åº”å†…å®¹ (å‰1000å­—ç¬¦):');
        console.log(response.content.substring(0, 1000));
        if (response.content.length > 1000) {
          console.log('... (è¿˜æœ‰', response.content.length - 1000, 'å­—ç¬¦)');
        }
        console.log('='.repeat(80) + '\n');
        
        return response;
      } catch (error: any) {
        lastError = error;
        console.warn(`âŒ LLM è°ƒç”¨å¤±è´¥ (å°è¯• ${i + 1}/${maxRetries + 1}):`, error.message);
        
        if (i < maxRetries) {
          // ç­‰å¾…åé‡è¯•
          await new Promise((resolve) => setTimeout(resolve, 1000 * (i + 1)));
        }
      }
    }
    
    throw lastError || new Error("LLM è°ƒç”¨å¤±è´¥");
  }
}

/**
 * åˆ›å»ºé»˜è®¤ LLM å®¢æˆ·ç«¯å®ä¾‹
 */
let defaultClient: LLMClient | null = null;

export function getDefaultLLMClient(): LLMClient {
  if (!defaultClient) {
    defaultClient = new LLMClient();
  }
  return defaultClient;
}

/**
 * åˆ›å»ºæ–°çš„ LLM å®¢æˆ·ç«¯å®ä¾‹
 */
export function createLLMClient(options?: LLMClientOptions): LLMClient {
  return new LLMClient(options);
}

